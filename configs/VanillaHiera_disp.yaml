dataset_ratio: 1.0
# 32 ~ 5 GB GPU
batch_size: 96
scale: 1.

model:
  name: HieraDPT
  out_chans: [3]
  pretrained_model: "exp/HieraRealMAE/checkpoints/last.ckpt"

  hiera:
    decoder: "Vanilla"
    decoder_embed_dim: 512
    decoder_num_heads: 16
    decoder_depth: 8

dataset:
  output_type: ["disp"]

optimizer:
  name: AdamW
  lr: 0.001

scheduler:
  name: linear_cosine
  warmup: 2000